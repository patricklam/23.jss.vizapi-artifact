\section{Design Decisions}
\label{sec:design-decisions}

Having described VizAPI's implementation, we now broaden our attention
and discuss possible design alternatives, particularly in terms of toolkits
that we could have used.

VizAPI works on Java bytecode. Java was a good choice for us
because the ecosystem of Java code available on the
Internet is vast, and Java bytecode is amenable to analysis. 
npm also has a lively ecosystem, but JavaScript is much harder to
analyze than Java, in part because it is much more dynamic. It is plausible
to us that WebAssembly will take over much of Java's niche in the medium-term
future, and it is also amenable to analysis.

As mentioned in Section~\ref{sec:implementation}, VizAPI presents both
static and dynamic information. Static analysis will typically present
over-approximations (e.g. both branches of a conditional might be
taken), except for under-approximations in special cases like
reflection and ``eval''. VizAPI makes almost maximally coarse
over-approximations, for instance by using Class Hierarchy Analysis to
approximate method calls. The soundiness
manifesto~\cite{livshits15:_in_defen_sound} discusses the
underapproximations used by typical static analysis in more detail.

On the other hand, dynamic analysis routinely under-approximates possible
program behaviours: it reports only the behaviour that is induced by
a particular program input. It completely reports that behaviour, though,
and sees through constructs like ``eval'' that are difficult for static analyses to
handle soundly. TamiFlex~\cite{bodden11:_tamin_reflec} dynamically captures
information about (among other things) which classes are loaded at runtime,
and feeds that back to the static analysis.

VizAPI integrates static and dynamic information. The approach is
still not sound with respect to all potential executions, and it does
not integrate dynamic class loading information into the static
results, but it does make both types of information (as 
captured by the tool) visible to the user.

\subsection{Static analysis and program transformation}
We next describe potential static analysis and transformation
approaches, from low-level bytecode manipulation through to
declaratively declaring desired analysis facts. VizAPI extracts static
facts from the programs under analysis. VizAPI also carries out some
program transformation so that it can collect dynamic facts.

Java bytecode is commonly used by program analysis tools as an input
format, since it is widely available and yet preserves the semantics
of the original source code. Unless obfuscated, it contains some
source-level entities such as classes and methods, with their original
names.

Although some source-level information information is present, it is
not complete. Original code structure (e.g. structured Abstract Syntax
Tree-level source-level loops rather than control-flow graphs) and
local variable names may not be present. However, the code structure
can be recovered, and local variable names can be inferred using Big
Code techniques similar to those used for JavaScript by Raychev et
al~\cite{raychev2016learning}.

Bytecode tools provide varying levels of abstraction. We discuss three
bytecode-level tools here, first from the perspective of fact extraction
and then from that of code manipulation and creation.

\paragraph{Low-level analyses}
The Apache Commons Byte Code Engineering Library\footnote{\url{https://commons.apache.org/proper/commons-bcel/}} (BCEL) is a low-level library that
provides a very thin abstraction layer to its user. For instance,
it requires users to manually manage constant pool entries and provides
verbatim access to stack-based bytecode instructions. In that sense,
users need to re-invent the wheel to extract higher-level information.

The ASM library\footnote{\url{https://asm.ow2.io/}} provides many of
the same capabilities as BCEL, but also a more robust
abstraction layer; e.g. ASM manages the constant pool
itself. In addition to an object-oriented (``tree-based'') API like
that of BCEL, it also provides an event-based API which trades increased
performance for decreased flexibility for the user (they must write
their analysis code in a certain way).

We chose to use
Javassist\footnote{\url{https://www.javassist.org/}}~\cite{chiba00:_load_struc_reflec_java}
for VizAPI. It provides more of an abstraction layer than BCEL (no
constant pool) but fewer abstractions than ASM (only one API, not
two). For our purposes, any of the libraries would have
worked, though BCEL would have required more effort to use. 

At a higher (non-bytecode) level, the Soot
framework~\cite{lam11:_soot_java} provides more support for
sophisticated intraprocedural analyses: instead of having to work with
stack-based bytecode, it transforms the input bytecode into a typed
three-address code, with explicit arguments for instructions. Soot
also provides a framework for writing intraprocedural dataflow
analyses.  At the intraprocedural level, because of the three-address
code and the dataflow analysis framework, writing a sophisticated
analysis is far easier in Soot than in bytecode-level libraries. On
the other hand, transforming the code to three-address code involves
more computational overhead.

To provide a concrete example, it is relatively easy to detect method
invocations in all of these frameworks. It is more difficult to, for
instance, detect method invocations with constant parameters, because
the analysis has to reason about the stack contents at the invocation
site. Soot's three-address code hides the stack from the user. In VizAPI's
case, we only need to detect the method invocation, which is easy
with any framework.

\paragraph{Low-level transformation}
While VizAPI's use case includes separate transformation and execution
phases, some tools transform code on-the-fly. All of BCEL, ASM,
and Javassist allow their users to transform code just before executing it,
in the same Virtual Machine. Soot was not designed to support on-the-fly
transformations.

In terms of generating code, the frameworks vary in their API support.
ASM and BCEL provide APIs that require the user to specify the exact
bytecode instructions to be used. Soot allows the user to provide
three-address code instructions by (somewhat awkwardly) constructing
them through its API. Javassist provides a ``simple Java compiler''
which can compile some code fragments into bytecode; the compiler does
not support all Java constructs.  The ByteBuddy
toolkit\footnote{\url{https://bytebuddy.net/}} builds on ASM but
supports a more declarative way of specifying code, e.g. ``create a
method that returns X''.

\paragraph{Interprocedural considerations}
More sophisticated static analysis can benefit from interprocedural
information. For instance, it is useful to know about possible
definition points for a local variable, even when that variable
was assigned from a method parameter and ultimately another method.

Unfortunately, in Java, precise interprocedural analysis requires a
call graph and pointer analysis information, and, as pointed out by
Lhot\'ak, call graphs and pointer analysis information must be
computed in tandem. Getting more precise results than Class Hierarchy
Analysis or, perhaps, Rapid Type
Analysis~\cite{bacon96:_fast_static_analy_c_virtual_funct_calls},
requires significant computation, and most analysis users would
benefit from using an off-the-shelf implementation. Soot's SPARK
toolkit~\cite{lhot02} is still used today, and
Doop~\cite{bravenboer09:_stric_declar_specif_sophis_point_analy}
provides more sophisticated options, which we discuss shortly.

Assuming that we have a call graph available, the next challenge in
designing a tool is to correctly propagate data interprocedurally.
Even with Soot's call graph information, it is still not that easy to
do whole-program analyses: it is up to the user to manually propagate
information at method invocation sites.

Heros~\cite{bodden12:_inter_proced_data_flow_analy} implements the
IFDS/IDE framework for interprocedural analysis of Java code. Being
interprocedural, it is more complicated to use than the dataflow analysis
framework built into Soot. However, it provides a lot of scaffolding
that a user would otherwise have to create; the user can focus on the
essentials of the analysis.

To our knowledge, all call graph implementations for Java require
minutes of startup time, even if the call graph computation itself
could be relatively fast. Such analyses would not be appropriate for
use at runtime. (But, any runtime analysis could instead use
easy-to-collect dynamic information and could roll back unsound
assumptions that it might have made).

Also, note that these interprocedural analyses need to know about
program entry points.  For an application with a \texttt{main()}
function, this is straightforward.  For Web middleware or for a
library, this is more complicated, and the user will need to specify
entry points. VizAPI's test-based dynamic approach essentially uses
all test cases as potential entry points, and it is relatively
straightforward to generate a driver that invokes all test cases.

\paragraph{Declarative approaches}
So far, we have discussed analysis and transformation approaches that have
been primarily imperative, with ByteBuddy a minor exception. By contrast,
declarative approaches like Doop are concise and surprisingly powerful.
In Doop's declarative approach, it suffices to declare the rules that
define the analysis. For instance, this utility rule relates methods
that have the same name and descriptor but have different declaring types:

\begin{lstlisting}
.decl SameMethodExceptDeclaringType(m:Method, m0:Method)
SameMethodExceptDeclaringType(m, m0) :-
  Method_SimpleName(m, n),
  Method_SimpleName(m0, n),
  Method_Descriptor(m, d),
  Method_Descriptor(m0, d).
\end{lstlisting}

In practice, one challenge of using Doop is that it is not obvious to find
the relations that Doop provides to users; the documentation is
somewhat sparse.

In any case, Doop reads the input program and the analysis definition,
computes necessary pointer and callgraph information, and feeds the
facts to the Soufflé Datalog solver. It is fairly simple to read out the
Doop results. Doop does not support program transformation and it would
be challenging to apply Doop results to drive transformations. Doop also
has substantial runtime cost.

\subsection{Dynamic analysis}
As discussed in Section~\ref{subsec:dynamic}, VizAPI uses Java instrumentation APIs (\texttt{java.lang.instrument interface}) and Javassist
to transform the code before running the instrumented code as a Java agent to collect
dynamic data.

Java provides two different mechanisms for gathering dynamic information: Java agents and the Java Virtual Machine Tool Interface (JVMTI).

\paragraph{Java agents}
Java agents, along with Java instrumentation APIs (\texttt{java.lang.instrument interface}), allow tool builders to modify classes' bytecode.
Different libraries, such as ASM and Javassist, provide APIs for bytecode manipulation; having considered the options described above, we chose Javassist for VizAPI.  
The Java agent is attached to the JVM, which means it shares the heap and the garbage collector with the application being run.
Since the agent also runs in the JVM, Java agents are portable between different Virtual Machine implementations.
We use a Java agent for VizAPI's dynamic analysis because it allows for straightforward bytecode manipulation 
and also because our dynamic analysis focuses on API interactions. Memory management, for instance, would be harder to observe from within the Virtual Machine.

\paragraph{JVMTI}
JVMTI is a set of native APIs which provide the ability to inspect the JVM in multiple ways, including memory management, 
thread synchronization, and code manipulation.
JVMTI agents are native and do not reside within the JVM. 
They are thus a good choice for monitoring JVM internals such as memory and garbage collection without introducing overhead from the agent itself.
However, JVMTI agents are not portable between JVMs running on different hosts, since JVMTI is a native interface and calls outside the JVM.

\subsection{Visualization}
Although GraphViz\footnote{\url{https://graphviz.org/}} is the dominant
free graph layout software, we chose to use d3graph to present our graphs.
It is still a mostly-static representation of a single graph. However,
d3graph renders to HTML, and allows the viewer to rotate the view
and move nodes. We found it useful to be able
to change the viewing angle of more complicated graphs.

CRAIG: any comments on this section in particular? probably should be longer and compare to more alternatives

\subsection{Open design questions}
We point out some of the design decisions that we have embedded into VizAPI's
design and which could be examined in future work.
\begin{itemize}
\item Granularity: We chose Java packages as the level of granularity, and coalesce nodes from the same JAR file and with the same edges. One could have a different coalescing policy. An overly-fine granularity makes graphs that are hard to understand, while an overly-coarse granularity provides no information. Also, some sort of cross-cutting modularity mechanism might make more sense, for instance a filter for all classes that use the \texttt{Node} class defined in some library.
\item Dependency types: We have chosen what we believe is a fairly exhaustive set of ways that components can depend on each other. Some of these ways may turn out to be unimportant to developers, and we may have missed others.
\item Combining static and dynamic information: We currently treat static and dynamic information separately. One could imagine an analysis design where dynamic information feeds into static information à la TamiFlex. (Also, by default, we show both static and dynamic edges; users may hide either static or dynamic information.)
\end{itemize}
